
import json
import re
import argparse
from tqdm import tqdm

# å¯¼å…¥ math-verify ç›¸å…³æ¨¡å—
try:
    from math_verify.errors import TimeoutException
    from math_verify.metric import math_metric
    from math_verify.parser import ExprExtractionConfig, LatexExtractionConfig, StringExtractionConfig
except ImportError:
    print("To use Math-Verify, please install it first by running `pip install math-verify`.")
    exit(1)

def extract_boxed_content(text: str) -> str:
    """æå–æœ€åä¸€ä¸ª \boxed{} çš„å†…å®¹ï¼Œæ”¯æŒå¤šå±‚åµŒå¥—èŠ±æ‹¬å·"""
    pattern = r'boxed\s*\{'
    matches = list(re.finditer(pattern, text))
    
    if not matches:
        return "No \\boxed{} found"
    
    last_match = matches[-1]
    start_pos = last_match.end()
    
    brace_count = 1
    i = start_pos
    while i < len(text) and brace_count > 0:
        if text[i] == '{':
            brace_count += 1
        elif text[i] == '}':
            brace_count -= 1
        i += 1
    
    if brace_count == 0:
        return text[start_pos:i-1].strip()
    return "Unmatched braces in \\boxed{}"

def compute_score(model_output: str, ground_truth, timeout_score: float = 0) -> float:
    """
    ä½¿ç”¨ math-verify è®¡ç®—æ¨¡å‹è¾“å‡ºä¸æ ‡å‡†ç­”æ¡ˆçš„åŒ¹é…åˆ†æ•°
    """
    verify_func = math_metric(
        gold_extraction_target=(LatexExtractionConfig(), ExprExtractionConfig(), StringExtractionConfig()),
        pred_extraction_target=(ExprExtractionConfig(), LatexExtractionConfig(), StringExtractionConfig()),
    )
    ret_score = 0.0
    # ğŸ”¥ ä¿®å¤ï¼šå°† ground_truth è½¬æ¢ä¸ºå­—ç¬¦ä¸²
    ground_truth_str = str(ground_truth)
    # Wrap the ground truth in \boxed{} format for verification
    ground_truth_boxed = "\\boxed{" + ground_truth_str + "}"
    try:
        ret_score, _ = verify_func([ground_truth_boxed], [model_output])
    except TimeoutException:
        ret_score = timeout_score
    except Exception as e:
        print(f"Error in verification: {e}")
        ret_score = 0.0
    return ret_score


def evaluate_jsonl(file_path: str, output_file: str = None, show_samples: bool = True, export_errors: bool = True):
    """
    è¯»å– JSONL æ–‡ä»¶å¹¶è¯„ä¼°æ¨¡å‹å›ç­”çš„æ­£ç¡®ç‡
    
    Args:
        file_path: è¾“å…¥JSONLæ–‡ä»¶è·¯å¾„
        output_file: è¯„ä¼°ç»“æœJSONæ–‡ä»¶è·¯å¾„
        show_samples: æ˜¯å¦æ˜¾ç¤ºæ ·ä¾‹
        export_errors: æ˜¯å¦å¯¼å‡ºé”™è¯¯case
    """
    correct_count = 0
    total_count = 0
    error_count = 0
    results = []
    error_cases = []  # ğŸ”¥ æ–°å¢ï¼šå­˜å‚¨é”™è¯¯case
    
    print(f"Reading file: {file_path}")
    
    # è¯»å– JSONL æ–‡ä»¶
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    print(f"Total samples: {len(lines)}")
    print("Starting evaluation...")
    
    # å¤„ç†æ¯ä¸€è¡Œæ•°æ®
    for idx, line in enumerate(tqdm(lines, desc="Evaluating")):
        try:
            data = json.loads(line.strip())
            
            # è·å–æ¨¡å‹è¾“å‡ºå’Œæ ‡å‡†ç­”æ¡ˆ
            model_response = data.get('model_response', data.get('response', ''))
            ground_truth = data.get('gdt', data.get('ground_truth', data.get('answer', '')))
            
            if not model_response or not ground_truth:
                print(f"Warning: Missing data at line {idx + 1}")
                error_count += 1
                continue
            
            # è®¡ç®—åˆ†æ•°
            score = compute_score(model_response, ground_truth)
            
            # æå– boxed å†…å®¹ç”¨äºå±•ç¤º
            model_answer = extract_boxed_content(model_response)
            
            # è®°å½•ç»“æœ
            is_correct = score > 0.5  # math-verify è¿”å› 0 æˆ– 1
            if is_correct:
                correct_count += 1
            else:
                # ğŸ”¥ æ–°å¢ï¼šä¿å­˜é”™è¯¯caseçš„å®Œæ•´åŸå§‹æ•°æ®
                error_case = data.copy()  # ä¿ç•™åŸå§‹æ•°æ®çš„æ‰€æœ‰å­—æ®µ
                error_case['_eval_info'] = {  # æ·»åŠ è¯„ä¼°ä¿¡æ¯
                    'score': score,
                    'extracted_answer': model_answer,
                    'ground_truth': ground_truth,
                    'line_number': idx + 1
                }
                error_cases.append(error_case)
            
            total_count += 1
            
            results.append({
                'index': idx,
                'score': score,
                'is_correct': is_correct,
                'model_answer': model_answer,
                'ground_truth': ground_truth,
                'original_data': data  # ğŸ”¥ ä¿å­˜åŸå§‹æ•°æ®
            })
            
        except json.JSONDecodeError:
            print(f"Error: Invalid JSON at line {idx + 1}")
            error_count += 1
        except Exception as e:
            print(f"Error processing line {idx + 1}: {e}")
            error_count += 1
    
    # è®¡ç®—æ­£ç¡®ç‡
    accuracy = correct_count / total_count if total_count > 0 else 0
    
    # æ‰“å°ç»“æœç»Ÿè®¡
    print("\n" + "="*50)
    print("EVALUATION RESULTS")
    print("="*50)
    print(f"Total samples processed: {total_count}")
    print(f"Correct answers: {correct_count}")
    print(f"Wrong answers: {total_count - correct_count}")
    print(f"Errors/Skipped: {error_count}")
    print(f"Accuracy: {accuracy:.2%}")
    print("="*50)
    
    # ä¿å­˜è¯¦ç»†ç»“æœåˆ°æ–‡ä»¶
    if output_file is None:
        output_file = file_path.replace('.jsonl', '_evaluation.json')
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            'summary': {
                'total_samples': total_count,
                'correct_count': correct_count,
                'wrong_count': total_count - correct_count,
                'error_count': error_count,
                'accuracy': accuracy
            },
            'details': results[:10]  # ä¿å­˜å‰10ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nDetailed results saved to: {output_file}")
    
    # ğŸ”¥ æ–°å¢ï¼šå¯¼å‡ºé”™è¯¯caseåˆ°JSONL
    if export_errors and error_cases:
        error_file = file_path.replace('.jsonl', '_mathverify_error.jsonl')
        with open(error_file, 'w', encoding='utf-8') as f:
            for case in error_cases:
                f.write(json.dumps(case, ensure_ascii=False) + '\n')
        
        print(f"\nğŸ”¥ Exported {len(error_cases)} error cases to: {error_file}")
        print(f"   Error rate: {len(error_cases)/total_count*100:.2f}%")
    
    # æ˜¾ç¤ºæ ·ä¾‹ï¼ˆå¯é€‰ï¼‰
    if show_samples:
        # æ˜¾ç¤ºä¸€äº›æ­£ç¡®çš„æ¡ˆä¾‹
        print("\n" + "="*50)
        print("SAMPLE CORRECT PREDICTIONS")
        print("="*50)
        correct_cases = [r for r in results if r['is_correct']][:3]
        for i, case in enumerate(correct_cases, 1):
            print(f"\nCorrect Case {i}:")
            print(f"  Ground Truth: {case['ground_truth']}")
            print(f"  Model Answer: {case['model_answer']}")
        
        # æ˜¾ç¤ºä¸€äº›é”™è¯¯çš„æ¡ˆä¾‹
        print("\n" + "="*50)
        print("SAMPLE INCORRECT PREDICTIONS")
        print("="*50)
        wrong_cases = [r for r in results if not r['is_correct']][:3]
        for i, case in enumerate(wrong_cases, 1):
            print(f"\nIncorrect Case {i}:")
            print(f"  Ground Truth: {case['ground_truth']}")
            print(f"  Model Answer: {case['model_answer']}")
        
        # æ˜¾ç¤ºç­”æ¡ˆåˆ†å¸ƒ
        print("\n" + "="*50)
        print("ANSWER DISTRIBUTION (First 20 samples)")
        print("="*50)
        for i, result in enumerate(results[:20], 1):
            status = "âœ“" if result['is_correct'] else "âœ—"
            print(f"{i:3d}. [{status}] Model: {result['model_answer'][:50]:<50} | GT: {result['ground_truth'][:50]}")
    
    return accuracy, results, error_cases  # ğŸ”¥ è¿”å›error_cases

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Evaluate math problem solving results')
    parser.add_argument('--input', '-i', type=str, required=True, 
                        help='Path to input JSONL file')
    parser.add_argument('--output', '-o', type=str, default=None,
                        help='Path to output evaluation JSON file (default: input_file_evaluation.json)')
    parser.add_argument('--no-samples', action='store_true',
                        help='Do not show sample predictions')
    parser.add_argument('--no-export-errors', action='store_true',  # ğŸ”¥ æ–°å¢å‚æ•°
                        help='Do not export error cases to JSONL')
    
    args = parser.parse_args()
    
    # æ‰§è¡Œè¯„ä¼°
    accuracy, results, error_cases = evaluate_jsonl(
        args.input, 
        output_file=args.output,
        show_samples=not args.no_samples,
        export_errors=not args.no_export_errors  # ğŸ”¥ ä¼ é€’å‚æ•°
    )
    
    # ğŸ”¥ æ–°å¢ï¼šæ˜¾ç¤ºé”™è¯¯caseç»Ÿè®¡
    if error_cases:
        print(f"\nğŸ“Š Error cases exported: {len(error_cases)}")
        print(f"   File: {args.input.replace('.jsonl', '_mathverify_error.jsonl')}")
